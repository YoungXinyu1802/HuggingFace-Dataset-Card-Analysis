---
dataset_info:
  features:
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 57873983898.0
    num_examples: 11339007
  download_size: 32369771967
  dataset_size: 57873983898.0
---
# Wikipedia and OSCAR Turkish Dataset

ğŸ‘‹ Welcome to the "Wikipedia and OSCAR Turkish" Huggingface Repo!

ğŸ“š This repo contains a Turkish language dataset generated by merging Wikipedia and OSCAR cleaned Common Crawl. The dataset contains over 11 million examples with a single feature - text.

ğŸ“Š Here's some quick information about the dataset:

Features:

Name: text
Data type: string
Splits:

Name: train
Number of bytes: 57871917202.0
Number of examples: 11338408
Download size: 32.3 GB

Dataset size: 57.8 GB

ğŸ” This dataset can be useful for natural language processing tasks in Turkish language.

ğŸ“¥ To download the dataset, you can use the Hugging Face Datasets library. Here's some sample code to get started:

    from datasets import load_dataset

    dataset = load_dataset("musabg/wikipedia-oscar-tr")

ğŸ¤– Have fun exploring this dataset and training language models on it!